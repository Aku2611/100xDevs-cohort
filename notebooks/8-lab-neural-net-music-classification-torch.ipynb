{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDaKPmEYtPa"
      },
      "source": [
        "# Assignment: Neural Networks for Music Classification\n",
        "\n",
        "*Fraida Fund*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceiK3LPkYtPb"
      },
      "source": [
        "**TODO**: Edit this cell to fill in your NYU Net ID and your name:\n",
        "\n",
        "-   **Net ID**:ax2135\n",
        "-   **Name**:Ankit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28qyp-V9YtPb"
      },
      "source": [
        "‚ö†Ô∏è **Note**: This experiment is designed to run on a Google Colab **GPU** runtime. You should use a GPU runtime on Colab to work on this assignment. You should not run it outside of Google Colab. However, if you have been using Colab GPU runtimes a lot, you may be alerted that you have exhausted the ‚Äúfree‚Äù compute units allocated to you by Google Colab. We will have some limited availability of GPU time during the last week before the deadline, for students who have no compute units available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SHhs1P8YtPb"
      },
      "source": [
        "In this assignment, we will look at an audio classification problem. Given a sample of music, we want to determine which instrument (e.g. trumpet, violin, piano) is playing.\n",
        "\n",
        "*This assignment is closely based on one by Sundeep Rangan, from his [IntroML GitHub repo](https://github.com/sdrangan/introml/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY1eOYdhYtPc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR-4IF3JYtPd"
      },
      "source": [
        "## Audio Feature Extraction with Librosa\n",
        "\n",
        "The key to audio classification is to extract the correct features. The `librosa` package in python has a rich set of methods for extracting the features of audio samples commonly used in machine learning tasks, such as speech recognition and sound classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CK_6PpmYtPe"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import librosa.feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CSfZfAsYtPe"
      },
      "source": [
        "In this lab, we will use a set of music samples from the website:\n",
        "\n",
        "<http://theremin.music.uiowa.edu>\n",
        "\n",
        "This website has a great set of samples for audio processing.\n",
        "\n",
        "We will use the `wget` command to retrieve one file to our Google Colab storage area. (We can run `wget` and many other basic Linux commands in Colab by prefixing them with a `!` or `%`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9B4G1QkYtPe"
      },
      "outputs": [],
      "source": [
        "!wget \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/SopSax.Vib.pp.C6Eb6.aiff\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kh07vC-YtPe"
      },
      "source": [
        "Now, if you click on the small folder icon on the far left of the Colab interface, you can see the files in your Colab storage. You should see the ‚ÄúSopSax.Vib.pp.C6Eb6.aiff‚Äù file appear there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb2PBUuAYtPf"
      },
      "source": [
        "In order to listen to this file, we‚Äôll first convert it into the `wav` format. Again, we‚Äôll use a magic command to run a basic command-line utility: `ffmpeg`, a powerful tool for working with audio and video files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62N9K9jJYtPf"
      },
      "outputs": [],
      "source": [
        "aiff_file = 'SopSax.Vib.pp.C6Eb6.aiff'\n",
        "wav_file = 'SopSax.Vib.pp.C6Eb6.wav'\n",
        "\n",
        "!ffmpeg -y -i $aiff_file $wav_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zee4NMO8YtPf"
      },
      "source": [
        "Now, we can play the file directly from Colab. If you press the ‚ñ∂Ô∏è button, you will hear a soprano saxaphone (with vibrato) playing four notes (C, C#, D, Eb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0nXum0yYtPf"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(wav_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4kVANGGYtPf"
      },
      "source": [
        "Next, use `librosa` command `librosa.load` to read the audio file with filename `audio_file` and get the samples `y` and sample rate `sr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grc36kssYtPf"
      },
      "outputs": [],
      "source": [
        "y, sr = librosa.load(aiff_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPPHhdTuYtPg"
      },
      "source": [
        "Feature engineering from audio files is an entire subject in its own right. A commonly used set of features are called the Mel Frequency Cepstral Coefficients (MFCCs). These are derived from the so-called mel spectrogram, which is something like a regular spectrogram, but the power and frequency are represented in log scale, which more naturally aligns with human perceptual processing.\n",
        "\n",
        "You can run the code below to display the mel spectrogram from the audio sample.\n",
        "\n",
        "You can easily see the four notes played in the audio track. You also see the 'harmonics' of each notes, which are other tones at integer multiples of the fundamental frequency of each note."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR_TSQT1YtPg"
      },
      "outputs": [],
      "source": [
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
        "                         y_axis='mel', fmax=8000, x_axis='time')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel spectrogram')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZlJXQm0YtPg"
      },
      "source": [
        "## Preparing the Data\n",
        "\n",
        "Using the MFCC features described above, [Prof.¬†Juan Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) at NYU Steinhardt and his former PhD student Eric Humphrey have created a complete data set that can used for instrument classification. Essentially, they collected a number of data files from the website above. For each audio file, the segmented the track into notes and then extracted 120 MFCCs for each note. The goal is to recognize the instrument from the 120 MFCCs. The process of feature extraction is quite involved. So, we will just use their processed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPdn5eQdYtPg"
      },
      "source": [
        "To retrieve their data, visit\n",
        "\n",
        "<https://github.com/marl/dl4mir-tutorial/tree/master>\n",
        "\n",
        "and note the password listed on that page. Click on the link for ‚ÄúInstrument Dataset‚Äù, enter the password, click on `instrument_dataset` to open the folder, and download it. (You can ‚Äúdirect download‚Äù straight from this site, you don‚Äôt need a Dropbox account.) Depending on your laptop OS and on how you download the data, you may need to ‚Äúunzip‚Äù or otherwise extract the four `.npy` files from an archive.\n",
        "\n",
        "Then, upload the files to your Google Colab storage: click on the folder icon on the left to see your storage, if it isn‚Äôt already open, and then click on ‚ÄúUpload‚Äù.\n",
        "\n",
        "üõë Wait until *all* uploads have completed and the orange ‚Äúcircles‚Äù indicating uploads in progress are *gone*. (The training data especially will take some time to upload.) üõë"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdgbE8PYYtPg"
      },
      "source": [
        "Then, load the files with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRJlNON9YtPg"
      },
      "outputs": [],
      "source": [
        "Xtr = np.load('uiowa_train_data.npy')\n",
        "ytr = np.load('uiowa_train_labels.npy')\n",
        "Xts = np.load('uiowa_test_data.npy')\n",
        "yts = np.load('uiowa_test_labels.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN6Vgr-2YtPg"
      },
      "source": [
        "Examine the data you have just loaded in:\n",
        "\n",
        "-   How many training samples are there?\n",
        "-   How many test samples are there?\n",
        "-   What is the number of features for each sample?\n",
        "-   How many classes (i.e.¬†instruments) are there?\n",
        "\n",
        "Write some code to find these values and print them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-3yS3GkYtPg"
      },
      "outputs": [],
      "source": [
        "# TODO -  get basic details of the data\n",
        "# compute these values from the data, don't hard-code them\n",
        "n_tr    = ...\n",
        "n_ts    = ...\n",
        "n_feat  = ...\n",
        "n_class = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOGLPpovYtPg"
      },
      "outputs": [],
      "source": [
        "# now print those details\n",
        "print(\"Num training= %d\" % n_tr)\n",
        "print(\"Num test=     %d\" % n_ts)\n",
        "print(\"Num features= %d\" % n_feat)\n",
        "print(\"Num classes=  %d\" % n_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLBWaOVYtPg"
      },
      "source": [
        "Convert the data to Pytorch ‚Äútensor‚Äù format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7M-9ZWzYtPg"
      },
      "outputs": [],
      "source": [
        "Xtr_tensor = torch.tensor(Xtr, dtype=torch.float32)\n",
        "Xts_tensor = torch.tensor(Xts, dtype=torch.float32)\n",
        "ytr_tensor = torch.tensor(ytr, dtype=torch.long)\n",
        "yts_tensor = torch.tensor(yts, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvvPE6IAYtPg"
      },
      "source": [
        "Then, standardize the training and test data, `Xtr_tensor` and `Xts_tensor`, by removing the mean of each feature and scaling to unit variance. Save the standardized data as `Xtr_scale` and `Xts_scale`, respectively.\n",
        "\n",
        "Although you will scale both the training and test data, you should make sure that both are scaled according to the mean and variance statistics from the *training data only*.\n",
        "\n",
        "You can use `torch.mean()` and `torch.std()` to compute the statistics of the data - but make sure to specify the dimension, `dim`!\n",
        "\n",
        "<small>Standardizing the input data can make the gradient descent work better, by making the loss function ‚Äúeasier‚Äù to descend.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He_tRWIkYtPg"
      },
      "outputs": [],
      "source": [
        "# TODO - Standardize the training and test data\n",
        "Xtr_scale = ...\n",
        "Xts_scale = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyIetvrNYtPg"
      },
      "source": [
        "Use the standardized data to create a `TensorDataset`, then use that to create training and test `DataLoader`s. Use a batch size of 128. Shuffle the training data, but not the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na_9Tp62YtPg"
      },
      "outputs": [],
      "source": [
        "# TODO - Create the data loaders\n",
        "batch_size = 128\n",
        "train_ds = TensorDataset(Xtr_scale, ytr_tensor)\n",
        "test_ds = TensorDataset(Xts_scale, yts_tensor)\n",
        "# train_loader = DataLoader(...)\n",
        "# test_loader = DataLoader(...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4dBZguBjYtPg"
      },
      "source": [
        "## Building a Neural Network Classifier\n",
        "\n",
        "Following the example in the demos you have seen, you will define a neural network in Pytorch with:\n",
        "\n",
        "-   `nh=256` hidden units in a single dense hidden layer\n",
        "-   `sigmoid` activation at hidden units\n",
        "-   select the input and output shapes according to the problem requirements. Use the variables you defined earlier (`n_tr`, `n_ts`, `n_feat`, `n_class`) as applicable, rather than hard-coding numbers.\n",
        "-   your model output should be a logit.\n",
        "\n",
        "as a class named `InstrumentNet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f74WcF2RYtPh"
      },
      "outputs": [],
      "source": [
        "# TODO - define the InstrumentNet\n",
        "class InstrumentNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(InstrumentNet, self).__init__()\n",
        "        # fill in details here!\n",
        "\n",
        "    def forward(self, x):\n",
        "        # fill in details here!\n",
        "\n",
        "# model = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKXqG556YtPh"
      },
      "source": [
        "Then create `model` as an instance of this class, and move `model` to the GPU device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbkiu7vcYtPh"
      },
      "outputs": [],
      "source": [
        "# TODO - create the model and move to device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wjg7-3XYtPh"
      },
      "source": [
        "Create a `criterion` and an `optimizer` to train the model parameters.\n",
        "\n",
        "-   Use an appropriate loss function for the `criterion`, considering that it is a multi-class classification problem and the model output is a logit.\n",
        "-   For the `optimizer`, use the Adam optimizer with a learning rate of 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzQBBLXnYtPh"
      },
      "outputs": [],
      "source": [
        "# TODO - create criterion and optimizer\n",
        "# criterion = ...\n",
        "# optimizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao3NFxQlYtPh"
      },
      "source": [
        "Next, fill in the implementation of a training function `train_model` that trains your model for one epoch!\n",
        "\n",
        "Make sure to move each batch of data to `device` as you get it from the data loader. The function should return the training accuracy and training loss as estimated during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWFom970YtPh"
      },
      "outputs": [],
      "source": [
        "# TODO - Training function\n",
        "def train_model(model, criterion, optimizer, device, train_loader):\n",
        "\n",
        "    # fill in details here\n",
        "    return train_acc, train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq483axiYtPj"
      },
      "source": [
        "Also, define an `eval_model` function. This function should accept a data loader `eval_loader`, and should return the accuracy and loss on that data.\n",
        "\n",
        "Make sure to move each batch of data to `device` as you get it from the data loader. The function should return the training accuracy and training loss as estimated during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufl-6yNAYtPk"
      },
      "outputs": [],
      "source": [
        "# TODO - Evaluation function\n",
        "def eval_model(model, criterion, device, eval_loader):\n",
        "\n",
        "    # fill in details here\n",
        "    return eval_acc, eval_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKrt_VjmYtPk"
      },
      "source": [
        "Now, you will use `train_model` to train your model for 10 epochs. At the end of each epoch, use `eval_model` to evaluate your model on the test data. Your final accuracy should be greater than 99%.\n",
        "\n",
        "Print the training and test accuracy at the end of each epoch.\n",
        "\n",
        "Save the training and test accuracy and loss after each epoch, to fill in the `results` dictionary as indicated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdqz5vEJYtPk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO - fit model and save training history per epoch\n",
        "n_epochs = 10\n",
        "results = {\n",
        "    'train_acc': [],\n",
        "    'test_acc': [],\n",
        "    'train_loss': [],\n",
        "    'test_loss': [],\n",
        "}\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suOtYn8WYtPk"
      },
      "source": [
        "Plot the training and test loss values on the same plot. You should see that the training loss is steadily decreasing. Use the [`semilogy` plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogy.html) so that the y-axis is log scale.\n",
        "\n",
        "Make sure to label each axis, and each series (training vs.¬†validation/test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhGqZsf6YtPk"
      },
      "outputs": [],
      "source": [
        "# TODO - plot the training and validation loss in one plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ma7tnEpYtPl"
      },
      "source": [
        "## Varying training hyperparameters\n",
        "\n",
        "One challenge in training neural networks is the selection of the **training hyperparameters**, for example:\n",
        "\n",
        "-   learning rate\n",
        "-   learning rate decay schedule\n",
        "-   batch size\n",
        "-   optimizer-specific hyperparameters (for example, the `Adam` optimizer we have been using has `beta_1`, `beta_2`, and `epsilon` hyperparameters)\n",
        "\n",
        "and this challenge is further complicated by the fact that all of these training hyperparameters interact with one another.\n",
        "\n",
        "(Note: **training hyperparameters** are distinct from **model hyperparameters**, like the number of hidden units or layers.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Mdpp1GYtPl"
      },
      "source": [
        "Sometimes, the choice of training hyperparameters affects whether or not the model will find an acceptable set of weights at all - i.e.¬†whether the optimizer converges.\n",
        "\n",
        "It‚Äôs more often the case, though, that **for a given model**, we can arrive at a set of weights that have similar performance in many different ways, i.e.¬†with different combinations of optimizer hyperparameters.\n",
        "\n",
        "However, the *training cost* in both **time** and **energy** will be very much affected.\n",
        "\n",
        "In this section, we will explore these further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UwfXXisYtPl"
      },
      "source": [
        "In the next cell, we will repeat the process above, but we will repeat this in a loop, once for each of the learning rates shown in the vector `rates`. In each iteration of the loop:\n",
        "\n",
        "-   create a new instance of the `model` (using the class you defined earlier), and move it to `device`. Create a new `optimizer`. Use the Adam optimizer with the learning rate specific to this iteration\n",
        "-   call the training function `train_model` that you wrote earlier to train the model for 20 epochs (make sure you are training a *new* model in each iteration, and not *continuing* the training of a model created already outside the loop). At the end of each epoch, use the `eval_model` function that you wrote earlier to evaluate the model on the test set.\n",
        "-   save the history of training and test accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpg9G6lZYtPm"
      },
      "outputs": [],
      "source": [
        "# TODO - iterate over learning rates\n",
        "rates = [0.1, 0.01,0.001,0.0001]\n",
        "n_epochs = 20\n",
        "\n",
        "results = {}\n",
        "\n",
        "for lr in rates:\n",
        "    results[lr] = {\n",
        "        'train_acc': [],\n",
        "        'test_acc': [],\n",
        "        'train_loss': [],\n",
        "        'test_loss': []\n",
        "    }\n",
        "    # fill in the rest of the implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iukjL2kDYtPm"
      },
      "source": [
        "Plot the *training* loss vs.¬†the epoch number for all of the learning rates on one graph (use `semilogy` again), using a different color for each learning rate. You should see that the lower learning rates are more stable, but converge slower, while with a learning rate that is too high, the gradient descent may fail to move towards weights that decrease the loss function.\n",
        "\n",
        "Make sure to label each axis, and each series.\n",
        "\n",
        "**Comment on the results.** Given that all other optimizer hyperparameters are fixed, what is the effect of varying learning rate on the training process?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn28lzc0YtPm"
      },
      "outputs": [],
      "source": [
        "# TODO - plot showing the training process for different learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_kFqRxKYtPm"
      },
      "source": [
        "In the previous example, we trained each model for a fixed number of epochs. Now, we‚Äôll explore what happens when we vary the training hyperparameters, but train each model to the same validation **accuracy target**. We will consider:\n",
        "\n",
        "-   how much *time* it takes to achieve that accuracy target (‚Äútime to accuracy‚Äù)\n",
        "-   how much *energy* it takes to achieve that accuracy target (‚Äúenergy to accuracy‚Äù)\n",
        "-   and the *test accuracy* for the model, given that it is trained to the specified validation accuracy target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRvQVBHOYtPm"
      },
      "source": [
        "#### Energy consumption\n",
        "\n",
        "To do this, first we will need some way to measure the energy used to train the model. We will use [Zeus](https://ml.energy/zeus/overview/), a Python package developed by researchers at the University of Michigan, to measure the GPU energy consumption.\n",
        "\n",
        "First, install the package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzXSkdxAYtPm"
      },
      "outputs": [],
      "source": [
        "!pip install zeus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPLge8DFYtPm"
      },
      "source": [
        "Then, import it, and start an instance of a monitor, specifying the GPU that it should monitor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgJtVpmIYtPm"
      },
      "outputs": [],
      "source": [
        "from zeus.monitor import ZeusMonitor\n",
        "\n",
        "monitor = ZeusMonitor(gpu_indices=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYnFCdN5YtPm"
      },
      "source": [
        "When you want to measure GPU energy usage, you will:\n",
        "\n",
        "-   start a ‚Äúmonitoring window‚Äù\n",
        "-   do your GPU-intensive computation (e.g.¬†call `train_model`)\n",
        "-   stop the ‚Äúmonitoring window‚Äù\n",
        "\n",
        "and then you can get the time and total energy used by the GPU in the monitoring window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuSB5Ox8YtPm"
      },
      "source": [
        "Try it now - call `train_model` to train whatever `model` is currently in scope from previous cells for one more epoch. Observe the measured time and energy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDM3iulKYtPm"
      },
      "outputs": [],
      "source": [
        "monitor.begin_window(\"test\")\n",
        "\n",
        "# call your train function here for one epoch\n",
        "\n",
        "measurement = monitor.end_window(\"test\")\n",
        "print(\"Measured time (s)  :\" , measurement.time)\n",
        "print(\"Measured energy (J):\" , measurement.total_energy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrbZtiTzYtPn"
      },
      "source": [
        "#### Train to Specified Accuracy\n",
        "\n",
        "Next, we need a way to train a model until we achieve our desired validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaVGRyPZYtPn"
      },
      "source": [
        "First, we need a validation set! If we ‚Äútrain until desired validation accuracy‚Äù, we are using the validation set to make decisions about model training. Therefore, we can‚Äôt use the test set as a validation set here. We need to split a separate validation set out of the test set.\n",
        "\n",
        "Use `random_split` to split out 20% of the training data for a validation set (as shown in e.g.¬†the Colab lesson on convolutional neural networks!).\n",
        "\n",
        "Then, create a new `train_loader` and a `val_loader` data loader. Use a batch size of 128 and shuffle the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9VNIofXYtPn"
      },
      "outputs": [],
      "source": [
        "# TODO - split out validation subset\n",
        "# train_ds_split, val_ds_split = random_split(...)\n",
        "# val_loader = ...\n",
        "# train_loader = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQBev_g-YtPn"
      },
      "source": [
        "Now, we can train to a specified accuracy!\n",
        "\n",
        "-   In the following cell, we define a `train_to_accuracy` function, which will accept the following arguments: a `max_epochs` value, a `threshold`, and a `patience` value.\n",
        "-   Inside the function, you will call `train_model` to train a model for up to `max_epochs`. At the end of each epoch, you will use `eval_model` to evaluate the model on the *validation* data (not the *test* data!)\n",
        "-   If the model‚Äôs validation accuracy is higher than the `threshold` for `patience` epochs in a row, stop training even if the `max_epochs` is not reached.\n",
        "-   At the end of training, use `eval_model` to evaluate the model on the test data.\n",
        "-   The default values of `max_epochs`, `threshold`, and `patience` are given below, but other values may be passed as arguments at runtime.\n",
        "\n",
        "Fill in the implementation below. You only need to return the final test data statistics - you don‚Äôt need to save these per epoch. You will also return the number of training epochs required to reach the desired accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SerpM4DXYtPn"
      },
      "outputs": [],
      "source": [
        "# TODO - define the train_to_accuracy function\n",
        "def train_to_accuracy(max_epochs = 100, threshold = 0.9, patience = 3)\n",
        "    # fill in details here\n",
        "    return test_acc, test_loss, epochs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wbK4GNDYtPn"
      },
      "source": [
        "Try it! Test your implementation. As some basic ‚Äúsanity checks‚Äù, make sure that:\n",
        "\n",
        "-   The smallest number of epochs it may use is `patience`, and the largest number is `max_epochs`\n",
        "-   The final training accuracy is at least `threshold`\n",
        "\n",
        "but also, check your implementation thoroughly and make sure it behaves as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhVl9ZfWYtPn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MPnDtYHYtPn"
      },
      "source": [
        "### See how TTA/ETA varies with learning rate, batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWCjugPYtPn"
      },
      "source": [
        "Now, you will repeat your model preparation and fitting code - with your new `train_to_accuracy` function - but in a loop. First, you will iterate over different learning rates.\n",
        "\n",
        "In each iteration, you will prepare a model (with the appropriate training hyperparameters) and train it until:\n",
        "\n",
        "-   either it has achieved **0.98 accuracy for 3 epoches in a row** on the 20% validation subset of the training data,\n",
        "-   or, it has trained for 100 epochs\n",
        "\n",
        "whichever comes FIRST.\n",
        "\n",
        "For each iteration, you will record:\n",
        "\n",
        "-   the training hyperparameters (learning rate, batch size)\n",
        "-   the number of epochs of training needed to achieve the target validation accuracy\n",
        "-   the accuracy on the *test* data (not the validation data!)\n",
        "-   the GPU energy and time to train the model to the desired validation accuracy, as computed by a `zeus` measurement window that starts just before the call to `train_to_accuracy` and ends just afterward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i16UEexYtPn"
      },
      "outputs": [],
      "source": [
        "# TODO - iterate over learning rates and get TTA/ETA\n",
        "\n",
        "# default learning rate and batch size -\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "metrics_vs_lr = []\n",
        "for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
        "\n",
        "\n",
        "    # TODO - set up model\n",
        "    # set up optimizer with specified learning rate\n",
        "\n",
        "    # start measurement\n",
        "    # call train_to_accuracy\n",
        "    # end measurement\n",
        "\n",
        "    # save results in a dictionary\n",
        "    # model_metrics = {\n",
        "    #    'batch_size': ...,\n",
        "    #    'learning_rate': ...,\n",
        "    #    'epochs': ...,\n",
        "    #    'test_accuracy': ...,\n",
        "    #    'test_loss': ...,\n",
        "    #    'total_energy': ...,\n",
        "    #    'train_time': ...\n",
        "    # }\n",
        "\n",
        "    # TODO - append model_metrics dictionary to the metrics_vs_lr list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFeDtlYvYtPo"
      },
      "source": [
        "Look at the output and make sure it is reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBZBc4s3YtPo"
      },
      "outputs": [],
      "source": [
        "metrics_vs_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGexp8itYtPo"
      },
      "source": [
        "Next, you will visualize the results.\n",
        "\n",
        "Create a figure with four subplots. In each subplot, create a bar plot with learning rate on the horizontal axis and (1) Time to accuracy, (2) Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis on each subplot, respectively. Use an appropriate vertical range for each subplot. Label all axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMBdC5ysYtPo"
      },
      "outputs": [],
      "source": [
        "# TODO - visualize effect of varying learning rate, when training to a target accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezlukubhYtPo"
      },
      "source": [
        "**Comment on the results**: Given that the model is trained to a target validation accuracy, what is the effect of the learning rate on the training process *in this example*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WVDqDpqYtPo"
      },
      "source": [
        "Now, you will repeat, with a loop over different batch sizes! This time, you will need to create a new `train_loader` inside each iteration. (You don‚Äôt have to change the data loader for the evaluation or test sets, since they won‚Äôt affect the training process.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSf5muKSYtPo"
      },
      "outputs": [],
      "source": [
        "# TODO - iterate over batch size and get TTA/ETA\n",
        "\n",
        "# default learning rate and batch size -\n",
        "lr = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "metrics_vs_bs = []\n",
        "for batch_size in [32,  128, 512, 2048]:\n",
        "\n",
        "    # TODO - set up model\n",
        "    # set up optimizer with specified learning rate\n",
        "    # set up a new train_loader with specified batch size\n",
        "    # (use the \"split\" of training data that does not include the validation set!)\n",
        "\n",
        "    # start measurement\n",
        "    # call train_to_accuracy\n",
        "    # end measurement\n",
        "\n",
        "    # save results in a dictionary\n",
        "    # model_metrics = {\n",
        "    #    'batch_size': ...,\n",
        "    #    'learning_rate': ...,\n",
        "    #    'epochs': ...,\n",
        "    #    'test_accuracy': ...,\n",
        "    #    'test_loss': ...,\n",
        "    #    'total_energy': ...,\n",
        "    #    'train_time': ...\n",
        "    # }\n",
        "\n",
        "    # TODO - append model_metrics dictionary to the metrics_vs_bs list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS45A94dYtPo"
      },
      "source": [
        "Next, you will visualize the results.\n",
        "\n",
        "Create a figure with four subplots. In each subplot, create a bar plot with batch size on the horizontal axis and (1) Time to accuracy, (2) Energy to accuracy, (3) Test accuracy, (4) Epochs, on the vertical axis on each subplot, respectively. Use an appropriate vertical range for each subplot. Label all axes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfohToOYtPo"
      },
      "source": [
        "Look at the output and make sure it is reasonable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qSmEFN5YtPo"
      },
      "outputs": [],
      "source": [
        "metrics_vs_bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jj43Ff3YtPo"
      },
      "outputs": [],
      "source": [
        "# TODO - visualize effect of varying batch size, when training to a target accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X46M09ixYtPo"
      },
      "source": [
        "**Comment on the results**: Given that the model is trained to a target validation accuracy, what is the effect of the batch size on the training process *in this example*? What do you observe about how time and energy *per epoch* and number of epochs required varies with batch size?"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  }
}